{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download audio using pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/analyticalmonk/open_source/local_llm_stt_summarization/./jeff dean leaders connect bangalore/audio.mp4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "# Enter the URL of the YouTube video\n",
    "video_url = \"https://www.youtube.com/watch?v=lks-x8ZM554\"\n",
    "\n",
    "# Create a YouTube object\n",
    "yt = YouTube(video_url)\n",
    "\n",
    "# Get the audio stream\n",
    "audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "\n",
    "# Download the audio stream\n",
    "audio_directory = \"jeff dean leaders connect bangalore\"\n",
    "audio_stream.download(output_path=f\"./{audio_directory}\", filename=\"audio.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate transcript using Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U openai-whisper\n",
    "# sudo apt update && sudo apt install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/analyticalmonk/miniconda3/envs/local_llm_stt_summarization/lib/python3.10/site-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' For example, is an example of of of using AI at the back end. I think as we as we go forward AI will touch every sector, every industry pretty much every use case.', 'segments': [{'id': 0, 'seek': 0, 'start': 0.0, 'end': 5.0, 'text': ' For example, is an example of of of using AI at the back end.', 'tokens': [50364, 1171, 1365, 11, 307, 364, 1365, 295, 295, 295, 1228, 7318, 412, 264, 646, 917, 13, 50614], 'temperature': 0.0, 'avg_logprob': -0.2893678310305573, 'compression_ratio': 1.38135593220339, 'no_speech_prob': 0.041291121393442154}, {'id': 1, 'seek': 0, 'start': 5.0, 'end': 13.0, 'text': ' I think as we as we go forward AI will touch every sector, every industry pretty much every use case.', 'tokens': [50614, 286, 519, 382, 321, 382, 321, 352, 2128, 7318, 486, 2557, 633, 6977, 11, 633, 3518, 1238, 709, 633, 764, 1389, 13, 51014], 'temperature': 0.0, 'avg_logprob': -0.2893678310305573, 'compression_ratio': 1.38135593220339, 'no_speech_prob': 0.041291121393442154}], 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Specify the path to the downloaded audio file\n",
    "audio_file_path = f\"./{audio_directory}/audio.mp4\"\n",
    "\n",
    "# Generate the transcript\n",
    "transcript = model.transcribe(audio_file_path)\n",
    "# transcript = model.transcribe(\"audio_sample.mp4\")\n",
    "\n",
    "# Print the transcript\n",
    "print(transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_llm_stt_summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
